{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_html(obj):\n",
    "    # shamelessly stolen\n",
    "    \n",
    "    if obj != None:\n",
    "        string = obj.encode_contents().decode('utf-8').strip()\n",
    "    else:\n",
    "        string = \"\"\n",
    "    return string\n",
    "\n",
    "def company_link(object_child):\n",
    "    # filter out link\n",
    "    \n",
    "    c_content = object_child.find_all('a')\n",
    "    if c_content != None and len(c_content) > 2:\n",
    "        return c_content[1][\"href\"]\n",
    "    else:\n",
    "        return []\n",
    "        \n",
    "def company_desc(object_child):\n",
    "    # inner html of description 'p' tag\n",
    "\n",
    "    page_text = object_child.find('p')\n",
    "    return inner_html(page_text)\n",
    "\n",
    "def strip_tag(string):\n",
    "    # return name of company, description\n",
    "    \n",
    "    n = string.count('>')/2\n",
    "#     n = 1\n",
    "#     print(n)\n",
    "    result = string.split('>',2*n)\n",
    "    if len(result) < 2*n + 1:\n",
    "        return \"\", string\n",
    "    else:\n",
    "        return result[n].split('<')[0], result[2*n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.eranyc.com/companies/\"\n",
    "page_response = requests.get(link)\n",
    "\n",
    "content = BeautifulSoup(page_response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 results found. filtering out linkless (my bad, its easier to discard links that don't work)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = list()\n",
    "# item_class=\"panel__description\"\n",
    "item_class=\"panel__wysiwyg\"\n",
    "parent = content.find_all('div', class_=item_class)\n",
    "\n",
    "for i, child in enumerate(parent):\n",
    "    listing = {}\n",
    "    \n",
    "    if child != None:\n",
    "        a_href = company_link(child)\n",
    "        desc   = company_desc(child)\n",
    "        name, desc = strip_tag(desc)\n",
    "        desc = name + desc\n",
    "        \n",
    "        listing['a'] = a_href\n",
    "        listing['name'] = name\n",
    "        listing['desc'] = desc\n",
    "#         listing['full'] = child\n",
    "\n",
    "        results.append(listing)\n",
    "\n",
    "#         if i < 10:\n",
    "#             print(listing)\n",
    "print(str(len(results)) + \" results found. filtering out linkless \"\n",
    "      \"(my bad, its easier to discard links that don't work)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 good\n",
      "50 bad\n",
      "{'a': u'http://www.agolo.com', 'name': u' helps readers get to the point faster, by applying the world\\u2019s most advanced summarization software to news, chat, voice, and video. The company\\u2019s AI platform can analyze thousands of documents and produce a summary of key points, specific to the reader\\u2019s interests, in real-time. Agolo has the largest dataset of summaries in existence and this powers their AI engine.', 'desc': u' helps readers get to the point faster, by applying the world\\u2019s most advanced summarization software to news, chat, voice, and video. The company\\u2019s AI platform can analyze thousands of documents and produce a summary of key points, specific to the reader\\u2019s interests, in real-time. Agolo has the largest dataset of summaries in existence and this powers their AI engine.In a world overwhelmed by content, Agolo enables efficient consumption so readers can spend more time thinking.'}\n"
     ]
    }
   ],
   "source": [
    "def condition(l):\n",
    "    return (\n",
    "        l['name'] != \"\" and\n",
    "        type(l['a']) != type([]) and\n",
    "        l['name'][:20] != l['desc'][:20]\n",
    "    )\n",
    "\n",
    "    \n",
    "easy = [l for l in results if condition(l)]\n",
    "\n",
    "hard = [l for l in results if not condition(l)]\n",
    "\n",
    "print(str(len(easy)) + \" good\")\n",
    "print(str(len(hard)) + \" bad\")\n",
    "\n",
    "print(hard[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting to csv\n"
     ]
    }
   ],
   "source": [
    "print(\"exporting to csv\")\n",
    "\n",
    "headers = ['name','desc','a']    \n",
    "rows = [[x['name'].encode('utf-8'),x['desc'].encode('utf-8'),x['a'].encode('utf-8')] for x in easy]\n",
    "\n",
    "with open('result.csv','w+') as csvfile:\n",
    "    # creating a csv writer object \n",
    "    csvwriter = csv.writer(csvfile) \n",
    "      \n",
    "    # writing the fields \n",
    "    csvwriter.writerow(headers) \n",
    "      \n",
    "    # writing the data rows \n",
    "    csvwriter.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
